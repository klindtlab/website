---
abstract: A recent line of work has shown promise in using sparse autoencoders (SAEs)
  to uncover interpretable features in neural network representations. However, the
  simple linear-nonlinear encoding mechanism in SAEs limits their ability to perform
  accurate sparse inference. Using compressed sensing theory, we prove that an SAE
  encoder is inherently insufficient for accurate sparse inference, even in solvable
  cases. We then decouple encoding and decoding processes to empirically explore conditions
  where more sophisticated sparse inference methods outperform traditional SAE encoders.
  Our results reveal substantial performance gains with minimal compute increases
  in correct inference of sparse codes. We demonstrate this generalises to SAEs applied
  to large language models, where more expressive encoders achieve greater interpretability.
  This work opens new avenues for understanding neural network representations and
  analysing large language model activations.
authors: Charles O'Neill and Alim Gumran and David Klindt
citations: 2
journal: ''
layout: publication
scholar_url: https://arxiv.org/abs/2411.13117
title: Compute Optimal Inference and Provable Amortisation Gap in Sparse Autoencoders
year: 2024
---

A recent line of work has shown promise in using sparse autoencoders (SAEs) to uncover interpretable features in neural network representations. However, the simple linear-nonlinear encoding mechanism in SAEs limits their ability to perform accurate sparse inference. Using compressed sensing theory, we prove that an SAE encoder is inherently insufficient for accurate sparse inference, even in solvable cases. We then decouple encoding and decoding processes to empirically explore conditions where more sophisticated sparse inference methods outperform traditional SAE encoders. Our results reveal substantial performance gains with minimal compute increases in correct inference of sparse codes. We demonstrate this generalises to SAEs applied to large language models, where more expressive encoders achieve greater interpretability. This work opens new avenues for understanding neural network representations and analysing large language model activations.
