---
abstract: 'Understanding how information is represented in neural networks is a fundamental
  challenge in both neuroscience and artificial intelligence. Despite their nonlinear
  architectures, recent evidence suggests that neural networks encode features in
  superposition, meaning that input concepts are linearly overlaid within the network''s
  representations. We present a perspective that explains this phenomenon and provides
  a foundation for extracting interpretable representations from neural activations.
  Our theoretical framework consists of three steps: (1) Identifiability theory shows
  that neural networks trained for classification recover latent features up to a
  linear transformation. (2) Sparse coding methods can extract disentangled features
  from these representations by leveraging principles from compressed sensing. (3)
  Quantitative interpretability metrics provide a means to assess the success of these
  methods, ensuring that extracted features align with human-interpretable concepts.
  By bridging insights from theoretical neuroscience, representation learning, and
  interpretability research, we propose an emerging perspective on understanding neural
  representations in both artificial and biological systems. Our arguments have implications
  for neural coding theories, AI transparency, and the broader goal of making deep
  learning models more interpretable.'
authors: David Klindt and Charles O'Neill and Patrik Reizinger and Harald Maurer and
  Nina Miolane
citations: 1
journal: ''
layout: publication
scholar_url: https://arxiv.org/abs/2503.01824
title: 'From superposition to sparse codes: interpretable representations in neural
  networks'
year: 2025
---

Understanding how information is represented in neural networks is a fundamental challenge in both neuroscience and artificial intelligence. Despite their nonlinear architectures, recent evidence suggests that neural networks encode features in superposition, meaning that input concepts are linearly overlaid within the network's representations. We present a perspective that explains this phenomenon and provides a foundation for extracting interpretable representations from neural activations. Our theoretical framework consists of three steps: (1) Identifiability theory shows that neural networks trained for classification recover latent features up to a linear transformation. (2) Sparse coding methods can extract disentangled features from these representations by leveraging principles from compressed sensing. (3) Quantitative interpretability metrics provide a means to assess the success of these methods, ensuring that extracted features align with human-interpretable concepts. By bridging insights from theoretical neuroscience, representation learning, and interpretability research, we propose an emerging perspective on understanding neural representations in both artificial and biological systems. Our arguments have implications for neural coding theories, AI transparency, and the broader goal of making deep learning models more interpretable.
